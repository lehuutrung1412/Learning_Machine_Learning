{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS115_SVD_PCA",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lehuutrung1412/Learning_Machine_Learning/blob/main/Singular_Value_Decomposition_Principal_Component_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAQnq4q1rTUc"
      },
      "source": [
        "# Đồ án CS115\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bszEI6HjsNPe"
      },
      "source": [
        "##Singular Value Decomposition \r\n",
        "\r\n",
        "In Linear Algebra, we learnt about diagonalization : a square matrix $A \\in R^{n\\times n}$ is diagonalizable if exist a invertible matrix $P$ and a diagonal matrix $D$ so that:\r\n",
        " $$ \\mathbf{A} = \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} $$\r\n",
        "But it only happens in square matrix, not all kind of matrix. So **Singular Value Decomposition** (SVD) is a ***special matrix factorization*** method for *any* real matrix, which helps us a lot in working with matrix and big data: storing matrix, finding features in data reduction,dimesionality reduction, etc. It's considered as a foundation of Machine Learning, one of the most useful tool in numerical linear algebra numerical for data processing. It's also the basis of Principal Component Analysis(PCA) - a widely techniques for analysis high dimesional data. SVD is a basis of facial regcontion algorithm.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3dv7IYIuzAL"
      },
      "source": [
        "#### Introduction to SVD\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XIXJiYlR2YQ"
      },
      "source": [
        "The singular value decomposition of a matrix is usually referred to as the SVD.\r\n",
        "\r\n",
        "This is the final and best factorization of any matrix:\r\n",
        "\r\n",
        "$$\\mathbf{A}_{m \\times n} = \\mathbf{U}_{m \\times m}\\mathbf{\\Sigma}_{m \\times n} (\\mathbf{V}_{n \\times n})^T  ~~~~~(1)$$\r\n",
        "\r\n",
        "where $U$ is **orthonormal**,  $\\Sigma$ is almost a **diagonal** matrix - only contains real numbers ordered *descending* $\\sigma_{1,2,....m}$ in main **diagonal** line ,$V$ is also **orthonormal**\r\n",
        "\r\n",
        "<br/>\r\n",
        "<br/>\r\n",
        "\r\n",
        "The image below describes the SVD for matrix A in 2 cases: $m < n $ and $m > n$   \r\n",
        "\r\n",
        "<img src=\"https://machinelearningcoban.com/assets/26_svd/svd.png\" width=\"75%\" height=\"75%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj3yFd7fQmFh"
      },
      "source": [
        "####Caculating SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sfBd6yjR5Nk"
      },
      "source": [
        "Ignoring dimesion of the matrix, base on equation $(1)$ we can compute $AA^T$ like this:\r\n",
        "\r\n",
        "\\begin{eqnarray}\r\n",
        "\\mathbf{AA}^T &=& \\mathbf{U}\\mathbf{\\Sigma} \\mathbf{V}^T (\\mathbf{U}\\mathbf{\\Sigma} \\mathbf{V}^T)^T \\ \r\n",
        "&=& \\mathbf{U}\\mathbf{\\Sigma} \\mathbf{V}^T \\mathbf{V}\\mathbf{\\Sigma}^T\\mathbf{U}^T \\ \r\n",
        "&=& \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{\\Sigma}^T\\mathbf{U}^T = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{\\Sigma}^T\\mathbf{U}^{-1}  ~~~~~ (2)\r\n",
        "\\end{eqnarray}\r\n",
        "\r\n",
        "\r\n",
        "$\\Sigma \\Sigma^T$ is a diagonal matrix contains $\\sigma^2_{1},\\sigma^2_{2}...\\sigma^2_{m}$. These $\\sigma^2$ is egien values of $AA^T$. Cause $AA^T$ is a positive semi-definite matrix so its eigen values is not negative. Those $\\sigma_j$ is square root of $AA^T$ eigen values - also called as *singular values*. Column vectors of $U$ are eigen vectors of $AA^T$. We call these vectors are *left-singular vectors*.\r\n",
        "\r\n",
        "</br> \r\n",
        "\r\n",
        "In the otherhand, caculating $A^TA$ will look like this:\r\n",
        "\\begin{eqnarray}\r\n",
        "\\mathbf{AA}^T &=& (\\mathbf{U}\\mathbf{\\Sigma} \\mathbf{V}^T)^T \\mathbf{U}\\mathbf{\\Sigma} \\mathbf{V}^T \\ \r\n",
        "&=& \\mathbf{V}\\mathbf{\\Sigma}^T\\mathbf{U}^T   \\mathbf{U}\\mathbf{\\Sigma} \\mathbf{V}^T \\\r\n",
        "&=& \\mathbf{V}\\mathbf{\\Sigma}^T\\mathbf{\\Sigma}\\mathbf{V}^T =  \\mathbf{V}\\mathbf{\\Sigma}^T\\mathbf{\\Sigma}\\mathbf{V}^{-1} ~~~~~ (3)\r\n",
        "\\end{eqnarray}\r\n",
        "\r\n",
        "Similar to $AA^T$, column vectors of $V$ are eigen vectors of $A^TA$. We call these vectors are *right-singular vectors*.\r\n",
        "\r\n",
        "</br>\r\n",
        "\r\n",
        "From $(2)(3)$, we can get matrix $\\Sigma, U , V$ just by diagonalizing $A^TA$ or $AA^T$.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY8QnBCCavfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e89aba86-6912-47a2-bc09-774ad64623ff"
      },
      "source": [
        "import numpy as np\r\n",
        "from numpy import linalg\r\n",
        "\r\n",
        "m, n = 3, 5 #m.n dim\r\n",
        "A = np.random.randint(0,50,(m, n)) #random A matrix\r\n",
        "print(A)\r\n",
        "#caculating the SVD of A\r\n",
        "U, S, V = linalg.svd(A)\r\n",
        "\r\n",
        "#creating matrix S cause  linalg.svd just return Sigma as an array\r\n",
        "Smatrix = np.zeros((m,n))\r\n",
        "for i in range(np.linalg.matrix_rank(A)):\r\n",
        "    Smatrix[i,i] += S[i]\r\n",
        "\r\n",
        "print(U)\r\n",
        "print()\r\n",
        "print(Smatrix)\r\n",
        "print()\r\n",
        "print(V)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[24 39 38 20 13]\n",
            " [21 42  6 18 27]\n",
            " [24 24 39 18 32]]\n",
            "[[-0.61278965  0.25956921 -0.74639981]\n",
            " [-0.51434354 -0.84807673  0.12734433]\n",
            " [-0.59994964  0.4619412   0.65320039]]\n",
            "\n",
            "[[101.91680844   0.           0.           0.           0.        ]\n",
            " [  0.          27.43858371   0.           0.           0.        ]\n",
            " [  0.           0.          16.8845575    0.           0.        ]]\n",
            "\n",
            "[[-0.39156404 -0.58773442 -0.48834049 -0.3170534  -0.40279842]\n",
            " [-0.01798057 -0.52515227  0.83061418 -0.06410882 -0.17280606]\n",
            " [ 0.025908   -0.4787997  -0.12581386 -0.0520115   0.86691712]\n",
            " [-0.27306035 -0.21073582 -0.08043719  0.93299068 -0.0639273 ]\n",
            " [-0.87813502  0.32422925  0.2220459  -0.1489644   0.22860337]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDuGVGhTdTIo"
      },
      "source": [
        "But in practice, we dont compute both equation $(2)(3)$. We choose 1 equation, compute $U$ or $V$ and $\\Sigma$, then using the definition of SVD $(1)$ to compute the leftover.\r\n",
        "Example:\r\n",
        "\r\n",
        "$$A = \\begin{pmatrix} 5 & 5 \\\\ -1 & 7 \\end{pmatrix} $$\r\n",
        "\r\n",
        "We're using equation $(3)$\r\n",
        "$$ A^TA = \\begin{pmatrix} 5 & -1 \\\\ 5 & 7 \\end{pmatrix} \\begin{pmatrix} 5 & 5 \\\\ -1 & 7 \\end{pmatrix} = \\begin{pmatrix} 26 & 18 \\\\ 18 & 74 \\end{pmatrix} $$\r\n",
        "Diagonalizing $A^TA$, we got $\\Sigma, V$.\r\n",
        "$$det(A^TA - \\lambda I ) = 0$$\r\n",
        "$$ \\lambda^2 - 100\\lambda + 1600 = 0$$\r\n",
        "$$ (\\lambda - 20 )(\\lambda-80) = 0$$\r\n",
        "$$=> \\lambda = 80,20$$\r\n",
        "$with \\, \\lambda_1 = 80 $\r\n",
        "$$A^TA - 80I = \\begin{pmatrix} -54 & 18 \\\\ 18 & -6 \\end{pmatrix}$$\r\n",
        "$$ => v_1 = \\begin{pmatrix} 1\\\\ 3 \\end{pmatrix} $$\r\n",
        "\r\n",
        "$with \\, \\lambda_2 = 20 $\r\n",
        "$$A^TA - 20I = \\begin{pmatrix} 6 & 18 \\\\ 18 & 54 \\end{pmatrix}$$\r\n",
        "$$ => v_2 = \\begin{pmatrix} -3 \\\\ 1 \\end{pmatrix}  $$\r\n",
        "But $v1,v2$ is orthogonal already. Normalizing $v_1, v_2$ .So now, we got:\r\n",
        "\r\n",
        "$$ \\Sigma = \\begin{pmatrix} 4\\sqrt{5}& 0 \\\\ 0 & 2\\sqrt{5} \\end{pmatrix} ,\r\n",
        "V = \\begin{pmatrix} 1/\\sqrt{10} & -3/\\sqrt{10} \\\\ 3/\\sqrt{10} & 1/\\sqrt{10} \\end{pmatrix} $$\r\n",
        "\r\n",
        "Now we use the SVD defination:\r\n",
        "\r\n",
        "$$ A = U \\Sigma V^T $$\r\n",
        "$$ AV = U \\Sigma $$\r\n",
        "\r\n",
        "$$ AV = \\begin{pmatrix} 5 & 5 \\\\ -1 & 7 \\end{pmatrix} \r\n",
        "\\begin{pmatrix} 1/\\sqrt{10} & -3/\\sqrt{10} \\\\ 3/\\sqrt{10} & 1/\\sqrt{10} \\end{pmatrix} =  \\begin{pmatrix} 2\\sqrt{10} & -\\sqrt{10} \\\\ 2\\sqrt{10} & \\sqrt{10} \\end{pmatrix} = U \\Sigma$$\r\n",
        "\r\n",
        "$$=> U = AV\\Sigma^{-1}=  \\begin{pmatrix} 2\\sqrt{10} & -\\sqrt{10} \\\\ 2\\sqrt{10} & \\sqrt{10} \\end{pmatrix}\r\n",
        " \\begin{pmatrix} \\sqrt{5}/20 & 0 \\\\ 0 & \\sqrt{5}/10 \\end{pmatrix} =\r\n",
        " \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}\r\n",
        "  $$\r\n",
        "\r\n",
        "Conclusion, we got 3 matrix $U, \\Sigma, V$ as a result after SVD-factorization matrix $A$\r\n",
        "$$A = \\begin{pmatrix} 5 & 5 \\\\ -1 & 7 \\end{pmatrix} = U\\Sigma V^T $$\r\n",
        "\r\n",
        "$$U = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{pmatrix}\r\n",
        ", \\Sigma = \\begin{pmatrix} 1/\\sqrt{10} & -3/\\sqrt{10} \\\\ 3/\\sqrt{10} & 1/\\sqrt{10} \\end{pmatrix}\r\n",
        ", V =\\begin{pmatrix} 1/\\sqrt{10} & -3/\\sqrt{10} \\\\ 3/\\sqrt{10} & 1/\\sqrt{10} \\end{pmatrix} \r\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP1AvpDjQ2Y-"
      },
      "source": [
        "####Some special SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xltA8sLCRyZQ"
      },
      "source": [
        "In reality, data is very complex, its can be lots of entries and features. For example, human genetic data can be a sequence of thousand features and there are thousand of people - which is huge; finding \"eigen face\" of people for facial regconition, a gray picture contains 1 face, may have a size 400 x 400 pixel, having different lighting condition, enormous data like that can't be interpreted and analysed. Even though, after doing SVD with these data, its still large. Truncated SVD will help us solve this big data problem. We will store a part of the origin SVD, but still knowing the main features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyhOnaoFUdEi"
      },
      "source": [
        "###### *Compacted SVD*\r\n",
        "\r\n",
        "Equation $(1)$ can be written as below:\r\n",
        "  $$ \\mathbf{A} = \\sigma_1 \\mathbf{u}_1 \\mathbf{v}^T_1 + \\sigma_2\\mathbf{u}_2\\mathbf{v}_2^T + \\dots + \\sigma_r\\mathbf{u}_r\\mathbf{v}_r^T $$\r\n",
        "with every $u_i v_i^T, 1 \\leq i \\leq r$ is a rank-1 matrix.\r\n",
        "\r\n",
        "In this formula, $A$ only depends on *first r columns* of $U,V$ and *r* non-values in main diagonal line of $\\Sigma$. So we have a better decomposition called ***compacted SVD***: \r\n",
        "$$ \\mathbf{A} = {\\mathbf{U}}_r{\\Sigma}_r({\\mathbf{V}}_r)^T $$\r\n",
        "If our $A$ has a smaller rank than *columns and rows* of $A$, meaning $ r \\ll m,n$, we have benefit of compacted SVD in storing data.\r\n",
        "\r\n",
        "\r\n",
        "<img src=\"https://machinelearningcoban.com/assets/26_svd/svd_truncated.png\" width=\"75%\" height=\"50%\"/>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-L_deil1Mua"
      },
      "source": [
        "###### *Truncated SVD*\r\n",
        "Remembered, $\\sigma$ values in the main diagonal line of $\\Sigma$ is non-zero and ordered descending. In common, some first values of $\\sigma_i$ are large, remaining values are small and can be zero. We dont want to store all the values in the SVD. Then, we can approximate matrix  $A \\approx \\hat{A} $ equal to sum of $k < r$ rank 1 matrices\r\n",
        "\r\n",
        "$$\\mathbf{A} \\approx \\mathbf{\\hat{A} } = U_k \\Sigma_k V_k^T = \\sigma_1 \\mathbf{u}_1 \\mathbf{v}^T_1 + \\sigma_2\\mathbf{u}_2\\mathbf{v}_2^T + \\dots + \\sigma_k\\mathbf{u}_k\\mathbf{v}_k^T $$\r\n",
        "\r\n",
        "Pushing away $r-k$ small and non-zero values in *SVD* is called ___Truncated SVD___. The error of subtraction $A-A_k$ is caculated by the Frobineous norm of the subtraction.But we have a theorem for it. *The error will equal to total square of the cut-off eigen values in truncated SVD*.\r\n",
        "\r\n",
        "$$ ||\\mathbf{A} - \\mathbf{A}_k||_F^2 = \\sum_{i = k + 1}^r \\sigma_i^2 ~~~ (4)$$\r\n",
        "Proof: \r\n",
        "\\begin{eqnarray}\r\n",
        "    ||\\mathbf{A} - \\mathbf{A}_k||_F^2 & = & ||\\sum_{i = k + 1}^r \\sigma_i \\mathbf{u}_i\\mathbf{v}_i^T ||_F^2    & (5)\\ \\\\\r\n",
        "    & = & \\text{trace}\\left\\{ \\left(\\sum_{i = k + 1}^r \\sigma_i \\mathbf{u}_i\\mathbf{v}_i^T\\right)\r\n",
        "    \\left(\\sum_{j = k + 1}^r \\sigma_j \\mathbf{u}_j\\mathbf{v}_j^T\\right)^T\r\n",
        "    \\right\\} & (6) \\ \\\\\r\n",
        "    &=& \\text{trace}\\left\\{ \\sum_{i = k + 1}^r \\sum_{j = k + 1}^r \\sigma_i\\sigma_j \\mathbf{u}_i\\mathbf{v}_i^T \\mathbf{v}_j \\mathbf{u}_j^T\r\n",
        "    \\right\\} & (7) \\  \\\\\r\n",
        "    &=& \\text{trace}\\left\\{ \\sum_{i = k + 1}^r  \\sigma_i^2\\mathbf{u}_i\\mathbf{u}_i^T\r\n",
        "    \\right\\} & (8) \\  \\\\\r\n",
        "    &=& \\text{trace}\\left\\{ \\sum_{i = k + 1}^r  \\sigma_i^2\\mathbf{u}_i^T\\mathbf{u}_i\r\n",
        "    \\right\\} & (9) \\  \\\\\r\n",
        "    &=& \\text{trace}\\left\\{ \\sum_{i = k + 1}^r  \\sigma_i^2\r\n",
        "    \\right\\} & (10) \\  \\\\\r\n",
        "    & = & \\sum_{i = k + 1}^r \\sigma_i^2 & \r\n",
        "\\end{eqnarray}\r\n",
        "\r\n",
        "With $k=0$, we got: \r\n",
        "$$ ||\\mathbf{A}||_F^2 = \\sum_{i = 1}^r \\sigma_i^2~~~~ (11) $$\r\n",
        "Fron $(4)(11)$ we can infer:\r\n",
        "$$ \\frac{||\\mathbf{A} - \\mathbf{A}_k||_F^2}{||\\mathbf{A}||_F^2} = {\\frac{\\sum_{i = k + 1}^r \\sigma_i^2}{\\sum_{j = 1}^r \\sigma_j^2}} ~~~~ (12)\r\n",
        "$$\r\n",
        "Thus, *the errorr of this approximation is very small if the cut-off eigen values is negligible for comparing to first k eigen values*. The theorem $(4)$ is important for caculating how much information we want to store. From equation $(12)$, we can pick smallest $k$ for storing up to $y \\% $ information. We can call it a low-rank approximation.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXfWnfXA7Stm"
      },
      "source": [
        "###### *Best k-rank approximation*\r\n",
        "\r\n",
        "This course : <a href=\"https://www.cs.princeton.edu/courses/archive/spring12/cos598C/svdchapter.pdf\">SVD - Princeton Course</a> proving that $A_k$ is also a root in this optimization problem: \r\n",
        "\\begin{eqnarray}\r\n",
        "\\min_{\\mathbf{B}} &&||\\mathbf{A} - \\mathbf{B}||_F \\ ~~~~~~\r\n",
        "\\text{s.t.} ~~ \\text{rank}(\\mathbf{B}) = k ~~~~~~~~~~~~~~ ()\r\n",
        "\\end{eqnarray}\r\n",
        "\r\n",
        "In above proof, $||\\mathbf{A} - \\mathbf{A}_k||_F^2 = \\sum_{i = k + 1}^r \\sigma_i^2 ~~~~ (4)$. If we using 2-norm instead of Frobeneous norm (F-norm) to caculate the error, $A_k$ is also a root for it.\r\n",
        "\r\n",
        "\\begin{eqnarray}\r\n",
        "\\min_{\\mathbf{B}} &&||\\mathbf{A} - \\mathbf{B}||_2 \\ \r\n",
        "\\text{s.t.} && \\text{rank}(\\mathbf{B}) = k ~~~~~~~~~~~~~~ ()\r\n",
        "\\end{eqnarray}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JZJ0BRKSK4f"
      },
      "source": [
        "\r\n",
        "####Application of SVD\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvz0dGu4SQgA"
      },
      "source": [
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam et tellus tempor est facilisis tempus quis id dolor. Donec varius pharetra velit vel euismod. Sed et vehicula orci.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iECNKUrpSSs0"
      },
      "source": [
        "##Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J52I6kaScFI"
      },
      "source": [
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam et tellus tempor est facilisis tempus quis id dolor. Donec varius pharetra velit vel euismod. Sed et vehicula orci.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uJQzsOETIhu"
      },
      "source": [
        "####Some Linear Algebra for PCA\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_cFKDfDUD-u"
      },
      "source": [
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam et tellus tempor est facilisis tempus quis id dolor. Donec varius pharetra velit vel euismod. Sed et vehicula orci.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fel-DzyOUFNT"
      },
      "source": [
        "####Into PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pemfh3sCUn3C"
      },
      "source": [
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam et tellus tempor est facilisis tempus quis id dolor. Donec varius pharetra velit vel euismod. Sed et vehicula orci.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3aFSv-ZUozY"
      },
      "source": [
        "####PCA Caculating steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb4LrGaPU9He"
      },
      "source": [
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam et tellus tempor est facilisis tempus quis id dolor. Donec varius pharetra velit vel euismod. Sed et vehicula orci.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyN-6DJIVUh7"
      },
      "source": [
        "####Relation between PCA and SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4V84h2xVbxp"
      },
      "source": [
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam et tellus tempor est facilisis tempus quis id dolor. Donec varius pharetra velit vel euismod. Sed et vehicula orci.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaR0906xVcZ5"
      },
      "source": [
        "####Application and Example of PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2BAlbMuZPN3"
      },
      "source": [
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam et tellus tempor est facilisis tempus quis id dolor. Donec varius pharetra velit vel euismod. Sed et vehicula orci.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGz7ocWRZQr9"
      },
      "source": [
        ""
      ]
    }
  ]
}